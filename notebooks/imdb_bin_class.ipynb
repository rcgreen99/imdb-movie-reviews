{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset url: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "\n",
      "Number of reviews: 50000\n",
      "Number of unqiue reviews: 49582\n",
      "\n",
      "positive    24884\n",
      "negative    24698\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Our baseline accuracy is : 0.501876\n"
     ]
    }
   ],
   "source": [
    "# read in the data\n",
    "df = pd.read_csv(\"data/IMDB-Dataset.csv\")\n",
    "print(df.head())\n",
    "print(f\"\\nNumber of reviews: {len(df.index)}\")\n",
    "\n",
    "# remove duplicate rows\n",
    "df.drop_duplicates(subset=['review'], inplace=True)\n",
    "print(f\"Number of unqiue reviews: {len(df.index)}\\n\")\n",
    "\n",
    "# check the distribution of the labels\n",
    "print(df.sentiment.value_counts())\n",
    "\n",
    "# get baseline\n",
    "print(f\"\\nOur baseline accuracy is : {max(df.sentiment.value_counts(normalize=True)):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert labels to 0 and 1\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset class for our movie dataset\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_len=512):\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.data = dataframe\n",
    "        # self.reviews = self.data.review\n",
    "        # self.targets = self.data.sentiment\n",
    "        self.reviews = dataframe.review.to_numpy()\n",
    "        self.targets = dataframe.sentiment.to_numpy()\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoded_review = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            # return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded_review['input_ids'].flatten(),\n",
    "            'attention_mask': encoded_review['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertForSequenceClassification\n",
    "\n",
    "# create custom model\n",
    "class DistilBertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        # self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        print(pooled_output)\n",
    "        linear_output = self.pre_classifier(pooled_output)\n",
    "        # dropout_output = self.dropout(linear_output)\n",
    "        output = self.classifier(linear_output)\n",
    "        return self.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    print(\"Training started...\")\n",
    "\n",
    "    train, val = MovieDataset(train_data), MovieDataset(val_data)\n",
    "\n",
    "    print(f\"Training on {len(train)} samples\")\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=16, shuffle=True)\n",
    "\n",
    "    print(f\"Training on {len(train_dataloader)} batches\")\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    # if use_cuda:\n",
    "    #     model = model.cuda()\n",
    "    #     criterion = criterion.cuda()\n",
    "\n",
    "    epochs = 1\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"Starting epoch {}\".format(epoch))\n",
    "        train_acc = 0\n",
    "        train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            optimizer.zero_grad() # clear gradients\n",
    "            outputs = model(input_ids, attention_mask) # forward pass\n",
    "            loss = criterion(outputs, targets) # calculate loss\n",
    "            train_loss += loss.item() # add loss to train_loss\n",
    "            loss.backward() # backward pass\n",
    "            optimizer.step() # update weights\n",
    "            break # remove this to train on all batches\n",
    "\n",
    "        val_acc = 0\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_dataloader)}\")\n",
    "        print(f\"Val Loss: {val_loss/len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Training on 39665 samples\n",
      "Training on 2480 batches\n",
      "Starting epoch 0\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "model = DistilBertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
